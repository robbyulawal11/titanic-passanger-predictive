# -*- coding: utf-8 -*-
"""Klasifikasi_Penumpang_Titanic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u8RDUwZaqJ9o5AU7b3V91shZ3-_oeoxr

# Submission 1 - Predictive Analytics

---
### My Identity
- Name : Robiul Awal
- ID Dicoding : robiul_awal
- Domicile : Yogyakarta
- Email : robbyulawal11@gmail.com
- Linkedin : https://www.linkedin.com/in/robiul-awal11/

---
<h1><center> Predicts which Passengers Survived The Titanic Shipwreck </h1></center>

### Introduction
<p align = "justify"> The sinking of the Titanic is one of the most famous shipwrecks in history.

On April 15, 1912, during its maiden voyage, the RMS Titanic, widely considered to be “unsinkable,” sank after colliding with an iceberg. Unfortunately, there were not enough lifeboats for everyone on board, resulting in the deaths of 1,502 of the 2,224 passengers and crew.

While there is some luck involved in survival, it appears that some groups of people are more likely to survive than others.

In this project, a predictive model will be built to answer the question: “what types of people are more likely to survive?” using passenger data (e.g. name, age, gender, socioeconomic class, etc.).

### Objective
<p align = "justify"> The goal of this study is to predict whether a passenger survived the sinking of the Titanic or not. For each question in the test set, you must predict a value of 0 or 1 for that variable.

### Dataset
<p align = "justify"> This dataset includes passenger information such as name, age, gender, socio-economic class, etc. There are two datasets, one dataset titled train.csv and the other titled test.csv.

Train.csv will contain details of some of the passengers on the plane (891 to be exact) and most importantly, it will reveal whether they survived or not, also known as the “ground truth”.

The test.csv dataset contains similar information but does not reveal the “ground truth” for each passenger. The task at hand is to predict this outcome. Using the patterns found in the train.csv data, it will be predicted whether the other 418 passengers on the plane (found in test.csv) survived.

### Process
- <p align = "justify"> Data Collection: Data collection was done by downloading the dataset from the official reference source from Kaggle, namely at the following link https://www.kaggle.com/competitions/titanic/data.
- <p align = "justify"> Data Exploration and Analysis: Analyze data by visualizing data so that it can be easily understood and able to see the trends that occur.
- <p align = "justify"> Data preprocessing: cleaning data such as empty data, noise data or data that has no effect on data processing, and normalizing data.
- <p align = "justify"> Dataset Splitting: Splitting the dataset into training and testing sets for the model training process.
- <p align = "justify"> Model Building: Building a classification model using random forest, support vector machine, and K-Nearest Neighbor.
- <p align = "justify"> Model Training: Train the model on the training dataset by optimizing its parameters and weights so that it can recognize patterns in the data.
- <p align = "justify">Model Validation: Validate the model on a validation dataset to measure its performance and prevent overfitting.
- <p align = "justify">Evaluation and Tuning: Evaluate the model on the test dataset and adjust parameters if necessary.

Referensi
Will Cukierski. (2012). Titanic - Machine Learning from Disaster. Kaggle. https://kaggle.com/competitions/titanic

## 1. Importing Packages

---
    
| ⚡ Description: Importing Packages ⚡ |
| :--------------------------- |
| In this section the required packages are imported, and briefly discuss, the libraries that will be used throughout the analysis and modelling. |
"""

# Commented out IPython magic to ensure Python compatibility.
# Import library for data cleaning
import pandas as pd
pd.options.mode.chained_assignment = None  # Disable chaining warnings
import numpy as np  # NumPy for numerical computing
seed = 0
np.random.seed(seed)  # Setting seeds for reproducibility

# Import library for visualitation
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns  # Seaborn for statistical data visualization, setting visualization styles

# Import library for data preprocessing
from sklearn.preprocessing import  OneHotEncoder
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from sklearn.preprocessing import StandardScaler

# Import library for modelling and evaluation
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

"""## 2. Loading Data

---
    
| ⚡ Description: Loading the data ⚡ |
| :--------------------------- |
| In this section you are required to load data from local and then extract it to the storage directory. |

---
"""

data_train = pd.read_csv('train.csv')
data_test = pd.read_csv('test.csv')

data_train

"""> - Based on the output above, it can be seen that the train data has 891 samples (records or number of observations) or rows. - In addition, the train data has 12 columns or variables consisting of PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, and Embarked.

## 3. Exploratory Data Analysis (EDA)

---
    
| ⚡ Description: Exploratory Data Analysis ⚡ |
| :--------------------------- |
| These preprocessing steps aim to remove noise, convert text to a consistent format, and extract important features for further analysis. |

---
"""

data_train.info()

""">- Based on the output above, it can be seen that there are 5 variables of the int64 numeric data type.
- There are 2 variables of the float64 numeric data type.
- There are 5 variables of the character or object type.
"""

data_train.describe()

""">Based on the output above, it can be seen that the average age of Titanic passengers was 29.69 years."""

dataNull = data_train.isnull().sum()
dataNull

""">Based on the output above, it can be seen that there are 177 empty data in the Age variable, 697 data in the Cabin variable, and 2 data in the Embarked variable."""

# Checking for data duplication
any(data_train.duplicated())

""">Based on the output above, it means that there is no duplicate data in the Titanic dataset used in this project.

### Univariate Analysis
Unvariate analysis is performed to determine the distribution of each variable in the dataset. This process produces a visualization that illustrates the comparison of distributions on each variable in the dataset.
"""

numerical_features = ['Survived','Pclass', 'Age', 'SibSp', 'Parch', 'Fare']
categorical_features = ['Sex', 'Embarked']

feature = categorical_features[0]
count = data_train[feature].value_counts()
percent = 100*data_train[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

""">Based on the image above, it can be seen that there are more men than women."""

feature = categorical_features[1]
count = data_train[feature].value_counts()
percent = 100*data_train[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

""">Based on the image above, it can be seen that most passengers depart from port S (Shouthampton)."""

data_train.hist(bins=50, figsize=(20,15))
plt.show()

"""### Multivariate Analysis
Multivariate analysis is used to determine the correlation between features and targets, namely the prediction of passenger safety in the Titanic ship accident. In this process, temporary assumptions can be obtained regarding what characteristics have a higher life expectancy in the Titanic ship accident.
"""

cat_features = data_train.select_dtypes(include='object').columns.to_list()

for col in categorical_features:
  sns.catplot(x=col, y="Survived", kind="bar", dodge=False, height = 4, aspect = 3,  data=data_train, palette="Set3")
  plt.title("Rata-rata 'survived' Relatif terhadap - {}".format(col))

"""> Berdasarkan variabel kategori dapat disimpulkan bahwa karakteristik orang yang memiliki kemungkinan lebih untuk hidup ketika terjadi kecelakaan kapal Titanic adalah perempuan dan yang berangkat dari pelabuhan Cherbourg."""

# Observing the relationship between numeric features with the pairplot() function
sns.pairplot(data_train[numerical_features], diag_kind = 'kde')

""">It can be seen from the image above that each variable has a random correlation with other variables."""

plt.figure(figsize=(10, 8))
correlation_matrix = data_train[numerical_features].corr().round(2)

# To print the value in the box, use the not=True parameter
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

""">Based on the image above, it can be seen that each variable has a low correlation with other variables. Meanwhile, the most influential variable among others on passenger safety in the Titanic ship accident is the fare or passenger tariff.

## 4. Data Preparation

---
    
| ⚡ Description: Data preparation ⚡ |
| :--------------------------- |
| These preparation steps aim encoding caracter or object data, normalitation, and extract important features for further analysis. |

---

### Handling missing value and Noise
>After analyzing the variables, it is necessary to clean the dataset so that it can be processed properly. In this project, cleaning will be carried out on several variables. The first will be the deletion of rows that have empty data on the Age and Embarked variables.
"""

# Create a new DataFrame (clean_df) by removing rows that have missing values ​​(NaN) from data_train
clean_df = data_train.dropna(subset=['Age'])

clean_df = clean_df.dropna(subset=['Embarked'])

""">Then it was decided that several variables would be removed because they were noise or redundant that had no effect on the prediction model in this project. The data were unique data that were not factors in the life or death of passengers in the Titanic accident because they did not cause it to happen at all. The variables to be removed included, Passenger ID, name, ticket number, and cabin number,"""

# Removing noise from a dataset
clean_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], inplace=True, axis=1)

clean_df.info()

"""### Feature Encoding
To perform the process of encoding category features, one of the common techniques is the one-hot-encoding technique. The scikit-learn library provides this function to obtain new appropriate features so that they can represent category variables. In this project, there are two category variables, namely 'sex' and 'embarked'. This encoding process is carried out so that variables whose values ​​are objects can become numeric values ​​so that they can be processed. This encoding process is carried out with the get_dummies feature.
"""

# Initialize OneHotEncoder
one_hot_encoder = OneHotEncoder(sparse_output=False)

data_final = pd.concat([clean_df, pd.get_dummies(clean_df['Sex'], prefix='Sex').astype(int)],axis=1)
data_final = pd.concat([data_final, pd.get_dummies(data_final['Embarked'], prefix='Embarked').astype(int)],axis=1)
data_final.drop(['Sex','Embarked'], axis=1, inplace=True)
data_final.info()

data_final.head()

"""### Over Sampling
Over-sampling is a technique to balance the amount of data between the majority and minority classes in an imbalanced dataset. The method used for over-sampling in this project is RandomOverSampler from the imbalanced-learn library. This technique adds random copies of samples in the minority class until the number is balanced with the majority class. In this case, the oversampling process is done to balance the records that have survived values ​​0 and 1.
"""

X = data_final.drop(["Survived"],axis =1)
y = data_final["Survived"]

# Oversampling use RandomOverSampler
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)

"""### Splitting Data
`train_test_split()` is a function from the scikit-learn library that is used to split a dataset into two or more parts, such as a training set and a testing set. This separation is important in machine learning to evaluate model performance using data that the model never saw during training, thus providing a more realistic picture of how the model performs on unseen data.
"""

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.1, random_state = 42)

print(f'Total # of sample in whole dataset: {len(X_resampled)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""### Normalization
Normalization or standardization of data is the process of changing the scale of numeric features so that they have certain statistical properties, usually with the aim of making machine learning models more stable and accurate. The method used for normalization in this project is using StandardScaler() from the scikit-learn library.
"""

numerical_features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

X_train[numerical_features].describe().round(4)

X_train.head()

"""## 5. Training Model

---
    
| ⚡ Description: Training model ⚡ |
| :--------------------------- |
| In this section, we will carry out training data using the Support Vector Machine, Random Forest, and K-Nearest Neighbors model that has been created. |

---

### Support Vector Machine

Creating an SVM model with the best parameters.
"""

# create an SVC object and call the fit function to train the model
svm = SVC(kernel='rbf', C=128, gamma=2048)
svm.fit(X_train, y_train)

"""### Random Forest

Creating an random forest model with the best parameters.
"""

# Initialize RF and call the fit function to train the model
RF = RandomForestClassifier(class_weight='balanced', random_state=42)
RF.fit(X_train, y_train)

"""### K-Nearest Neighbors

Creation of K-Nearest Neighbors model with best parameters.
"""

# Initialize the KNN model with k=3 and call the fit function to train the model
knn = KNeighborsClassifier(n_neighbors=3,
    weights='distance',     # Assign weights based on distance (closer neighbors have greater weight)
    algorithm='auto',       # Automatically select algorithm based on data
    leaf_size=30,           # Leaf size for Ball Tree or KD Tree
    p=1,                    # Using Euclidean distance (p=2)
    metric='manhattan',     # Minkowski distance metric
    n_jobs=-1     )

knn.fit(X_train, y_train)

"""## 6. Evaluation Model

---
    
| ⚡ Description: Data preprocessing ⚡ |
| :--------------------------- |
| Model evaluation in classification models is very important to determine how well the model can predict the class of new data. This evaluation uses various metrics to assess the performance of the model, which includes the model's ability to correctly distinguish positive and negative classes. Here are some of the metrics used in this project, namely accuracy score, precision score, recall score, and f1-score.|

---
"""

# Scale the numeric features on X_test so that it has a mean=0 and variance=1
X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

X_test.head()

"""### Support Vector Machine
The following is the process for obtaining accuracy values ​​in the support vector machine model.
"""

# Predicting test data
y_pred_train_svm = svm.predict(X_train)
y_pred_test_svm = svm.predict(X_test)

# Evaluation using various metrics
accuracy_train_svm = accuracy_score(y_pred_train_svm, y_train)
accuracy_test_svm = accuracy_score(y_pred_test_svm, y_test)

# Showing accuracy
print('svm - accuracy_train:', accuracy_train_svm)
print('svm - accuracy_test:', accuracy_test_svm)

"""### Random Forest
The following is the process for obtaining accuracy values ​​in the random forest model.
"""

# Predicting test data
y_pred_train_RF = RF.predict(X_train)
y_pred_test_RF = RF.predict(X_test)

# Evaluation using various metrics
accuracy_train_RF = accuracy_score(y_pred_train_RF, y_train)
accuracy_test_RF = accuracy_score(y_pred_test_RF, y_test)

# Showing accuracy
print('RF - accuracy_train:', accuracy_train_RF)
print('RF - accuracy_test:', accuracy_test_RF)

"""### K-Naerest Neighbors
The following is the process for obtaining accuracy values ​​in the k-nearest neighbors model.
"""

# Predicting test data
y_pred_train_knn = knn.predict(X_train)
y_pred_test_knn = knn.predict(X_test)

# Evaluation using various metrics
accuracy_train_knn = accuracy_score(y_pred_train_knn, y_train)
accuracy_test_knn = accuracy_score(y_pred_test_knn, y_test)

# Showing accuracy
print('knn - accuracy_train:', accuracy_train_knn)
print('knn - accuracy_test:', accuracy_test_knn)

# Create a confusion matrix for each model
conf_matrix_rf = confusion_matrix(y_test, y_pred_test_RF)
conf_matrix_svm = confusion_matrix(y_test, y_pred_test_svm)
conf_matrix_knn = confusion_matrix(y_test, y_pred_test_knn)

# Displaying confusion matrix and classification report
print("Confusion Matrix - Random Forest:")
print(conf_matrix_rf)
print("\nClassification Report - Random Forest:")
print(classification_report(y_test, y_pred_test_RF))

print("\nConfusion Matrix - SVM:")
print(conf_matrix_svm)
print("\nClassification Report - SVM:")
print(classification_report(y_test, y_pred_test_svm))

print("\nConfusion Matrix - K-Nearest Neighbors:")
print(conf_matrix_knn)
print("\nClassification Report - K-Nearest Neighbors:")
print(classification_report(y_test, y_pred_test_knn))

# Create a DataFrame for accuracy results
results_df = pd.DataFrame({
    'Model': [ 'Support Vector Machines', 'Random Forest', 'K-Nearest Neighbor'],
    'Accuracy Train': [ accuracy_train_svm, accuracy_train_RF, accuracy_train_knn ],
    'Accuracy Test': [ accuracy_test_svm, accuracy_test_RF, accuracy_test_knn ]
})
# Showing only the "Accuracy Test" column
accuracy_test_only = results_df[['Model', 'Accuracy Test']]
print(accuracy_test_only)

""">After evaluating the best model, namely the random forest classification of Titanic passengers, the following results were obtained:
Obtained
- **Accuracy:** 91%
- **Precision for the "Survivor" Class:** 85%
- **Recall for the "Survivor" Class:** 97%
- **F1 Score for the "Survivor" Class:** 91%

>Explanation of the results based on these metrics:
- **Accuracy of 91%** indicates that the model correctly classifies 91% of the samples. However, considering that accuracy can be affected by class imbalance, we also need to look at precision and recall.
- **Precision of 85%** indicates that 85% of the passengers predicted to be "survivors" actually survived. This shows that the model is quite good at avoiding false positives (passengers who did not survive but were predicted to survive).
- **Recall of 97%** indicates that the model successfully identified 97% of all passengers who actually survived. The model is quite good, because there is only a chance that 3% of passengers who should have survived were not identified by the model (false negatives).
- **F1 Score 91%** provides a comprehensive picture of the balance between precision and recall. This shows that the model has a balanced performance in capturing survivors while minimizing prediction errors.

## 7. Prediction

---
    
| ⚡ Description: Prediction ⚡ |
| :--------------------------- |
|In this section, we will make predictions on new data to determine the probability of safety for each passenger based on existing factors or variables.|

---

### Loading data test
"""

data_test = pd.read_csv('test.csv')
data_test.head()

"""### Exploratory Data Analysis (EDA)"""

data_final = data_test.copy()
data_final.head()

"""> Removing noise from the dataset"""

# remove noise from dataset
data_final.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], inplace=True, axis=1)
data_final.head()

"""### Data Preparation

>Perform categorical data encoding on test data
"""

# Initialize OneHotEncoder
one_hot_encoder = OneHotEncoder(sparse_output=False)

data_test_final = pd.concat([data_final, pd.get_dummies(data_final['Sex'], prefix='Sex').astype(int)],axis=1)
data_test_final = pd.concat([data_test_final, pd.get_dummies(data_test_final['Embarked'], prefix='Embarked').astype(int)],axis=1)
data_test_final.drop(['Sex','Embarked'], axis=1, inplace=True)
data_test_final.info()

""">Perform normalization on test data"""

numerical_features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']
scaler = StandardScaler()
scaler.fit(data_test_final[numerical_features])
data_test_final[numerical_features] = scaler.transform(data_test_final.loc[:, numerical_features])
data_test_final.head()

"""### Prediction with The Best Model"""

# Melakukan prediksi pada seluruh data
data_test['prediction'] = RF.predict(data_test_final)

# Menyimpan DataFrame ke dalam file Excel
data_test.to_excel('hasil_prediksi.xlsx', index=False)

print("DataFrame dengan hasil prediksi:")
print(data_test.head())

"""# Export requirements.txt
This section will export the libraries used in this project in the form of a .txt file.
"""

pip freeze > requirements.txt